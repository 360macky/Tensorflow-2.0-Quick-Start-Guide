{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 1000 # number of training examples\n",
    "training_steps = 1000 # number of steps we are going to train for\n",
    "display_step = 100 # after multiples of this display the loss\n",
    "learning_rate = 0.01 # multipliying factor on gradient\n",
    "m, c = 6, -5 # gradient and y intercept of our line, edit these for a different linear problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataset of points around mx + c\n",
    "def train_data(n, m, c):\n",
    "  x = tf.random.normal([n]) # n values taken from a normal distribution, mean = 0, SD =1\n",
    "  noise = tf.random.normal([n])# n values taken from a normal distribution, mean = 0, SD =1\n",
    "  y = m*x + c + noise # our scatter plot\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(x, weight, bias):\n",
    "  return weight*x + bias # our predicted (learned)  m and c, expression is like y = m*x + c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A loss function using mean-squared error\n",
    "def loss(x, y, weights, biases):\n",
    "  error = prediction(x, weights, biases) - y #  how 'wrong' our predicted (learned)  y is\n",
    "  squared_error = tf.square(error)\n",
    "  return tf.reduce_mean(input_tensor=squared_error) # overall mean of squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the derivative of loss with respect to weight and bias\n",
    "def grad(x, y, weights, biases):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_ = loss(x, y, weights, biases)\n",
    "  return tape.gradient(loss_, [weights, biases]) # direction and value of the gradient of our loss w.r.t weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with random values for W and B on the same batch of data\n",
    "x, y = train_data(n_examples,m,c) # our training values x and  y\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Figure 1: Training Data\")\n",
    "W = tf.Variable(np.random.randn()) # initial, random, value for predicted weight (m)\n",
    "B = tf.Variable(np.random.randn()) # initial, random, value for predicted bias (c)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(x, y, W, B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(training_steps): #iterate for each training step\n",
    "    deltaW, deltaB = grad(x, y, W, B) # direction (sign)  and value of the gradient of our loss w.r.t weight and bias\n",
    "    change_W = deltaW * learning_rate # adjustment amount for weight\n",
    "    change_B = deltaB * learning_rate # adjustment amount for bias\n",
    "    W.assign_sub(change_W) # subract change_W from W\n",
    "    B.assign_sub(change_B) # subract change_B from B\n",
    "    if step==0 or step % display_step == 0:\n",
    "        # print(deltaW.numpy(), deltaB.numpy()) # uncomment if you want to see the gradients\n",
    "        print(\"Loss at step {:02d}: {:.6f}\".format(step, loss(x, y, W, B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final loss: {:.3f}\".format(loss(x, y, W, B)))\n",
    "print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))\n",
    "print(\"Compared with m = {:.1f}, c = {:.1f}\".format(m, c),\" of the original line\")\n",
    "xs = np.linspace(-3, 4, 50)\n",
    "ys = W.numpy()*xs + B.numpy()\n",
    "plt.scatter(xs,ys)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Figure 2: Line of Best Fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
